{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, 'I really liked this movie'),\n",
    "                            (2, 'I would recommend this movie to my friends'),\n",
    "                            (3, 'movie was alright but acting was horrible'),\n",
    "                            (4, 'I am never watching that movie ever again')], \n",
    "                           ['user_id', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|              review|\n",
      "+-------+--------------------+\n",
      "|      1|I really liked th...|\n",
      "|      2|I would recommend...|\n",
      "|      3|movie was alright...|\n",
      "|      4|I am never watchi...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|              review|              tokens|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|I really liked th...|[i, really, liked...|\n",
      "|      2|I would recommend...|[i, would, recomm...|\n",
      "|      3|movie was alright...|[movie, was, alri...|\n",
      "|      4|I am never watchi...|[i, am, never, wa...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenization = Tokenizer(inputCol='review', outputCol='tokens')\n",
    "tokenized_df = tokenization.transform(df)\n",
    "tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|              tokens|      refined_tokens|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|[i, really, liked...|[really, liked, m...|\n",
      "|      2|[i, would, recomm...|[recommend, movie...|\n",
      "|      3|[movie, was, alri...|[movie, alright, ...|\n",
      "|      4|[i, am, never, wa...|[never, watching,...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_removal = StopWordsRemover(inputCol='tokens', \n",
    "                                    outputCol='refined_tokens')\n",
    "refined_df = stopword_removal.transform(tokenized_df)\n",
    "refined_df.select(['user_id', 'tokens', 'refined_tokens']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|      refined_tokens|            features|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|[really, liked, m...|(11,[0,1,8],[1.0,...|\n",
      "|      2|[recommend, movie...|(11,[0,3,10],[1.0...|\n",
      "|      3|[movie, alright, ...|(11,[0,4,7,9],[1....|\n",
      "|      4|[never, watching,...|(11,[0,2,5,6],[1....|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(inputCol='refined_tokens', \n",
    "                            outputCol='features')\n",
    "cv_df = count_vec.fit(refined_df).transform(refined_df)\n",
    "cv_df.select(['user_id', 'refined_tokens', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'horrible',\n",
       " 'liked',\n",
       " 'alright',\n",
       " 'friends',\n",
       " 'really',\n",
       " 'watching',\n",
       " 'ever',\n",
       " 'recommend',\n",
       " 'acting',\n",
       " 'never']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(refined_df).vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|      refined_tokens|         tf_features|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|[really, liked, m...|(262144,[14,32675...|\n",
      "|      2|[recommend, movie...|(262144,[129613,1...|\n",
      "|      3|[movie, alright, ...|(262144,[80824,15...|\n",
      "|      4|[never, watching,...|(262144,[63139,15...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "\n",
    "hashing_vec = HashingTF(inputCol='refined_tokens',\n",
    "                        outputCol='tf_features')\n",
    "hashing_df = hashing_vec.transform(refined_df)\n",
    "hashing_df.select(['user_id', 'refined_tokens', 'tf_features']).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     tf_idf_features|\n",
      "+-------+--------------------+\n",
      "|      1|(262144,[14,32675...|\n",
      "|      2|(262144,[129613,1...|\n",
      "|      3|(262144,[80824,15...|\n",
      "|      4|(262144,[63139,15...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf_df.select(['user_id', 'tf_idf_features']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = spark.read.csv('Movie_reviews.csv', inferSchema=True, header=True, sep=',')\n",
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only records that are labeled correctly.\n",
    "\n",
    "text_df = text_df.filter(((text_df.Sentiment == '1') | (text_df.Sentiment == '0')))\n",
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0| 3081|\n",
      "|        1| 3909|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|oh and i loved th...|        1|\n",
      "|Da Vinci Code suc...|        0|\n",
      "|I want to be here...|        1|\n",
      "|The Da Vinci Code...|        1|\n",
      "|I love its Harry ...|        1|\n",
      "|Love luv lubb the...|        1|\n",
      "|These Harry Potte...|        0|\n",
      "|Harry Potter drag...|        0|\n",
      "|\"\"\" brokeback mou...|        0|\n",
      "|the last stand an...|        1|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Review|Label|\n",
      "+--------------------+-----+\n",
      "|Da Vinci Code suc...|  0.0|\n",
      "|i love kirsten / ...|  1.0|\n",
      "|i heard da vinci ...|  0.0|\n",
      "|I am going to sta...|  1.0|\n",
      "|Combining the opi...|  0.0|\n",
      "|the story of Harr...|  1.0|\n",
      "|I did lapse once,...|  0.0|\n",
      "|then we went to m...|  1.0|\n",
      "|i liked the Da Vi...|  1.0|\n",
      "|lol ya and then i...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = text_df.withColumn('Label', text_df.Sentiment.cast('float')).drop('Sentiment')\n",
    "text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|              Review|Label|length|\n",
      "+--------------------+-----+------+\n",
      "|Back in Melbourne...|  1.0|    72|\n",
      "|by the way, the D...|  0.0|    62|\n",
      "|Oh, and Brokeback...|  0.0|    48|\n",
      "|Friday I went out...|  1.0|    72|\n",
      "|friday hung out w...|  0.0|    72|\n",
      "|Which is why i sa...|  1.0|    72|\n",
      "|The Da Vinci Code...|  1.0|    71|\n",
      "|\"Anyway, thats wh...|  1.0|    49|\n",
      "|Oh, and Brokeback...|  0.0|    48|\n",
      "|Brokeback Mountai...|  1.0|    34|\n",
      "+--------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = text_df.withColumn('length', length(text_df['Review']))\n",
    "text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Label|      avg(Length)|\n",
      "+-----+-----------------+\n",
      "|  1.0|47.61882834484523|\n",
      "|  0.0|50.95845504706264|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Label').agg({'Length': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and stopwords removal.\n",
    "tokenization = Tokenizer(inputCol='Review', \n",
    "                         outputCol='tokens')\n",
    "tokenized_df = tokenization.transform(text_df)\n",
    "stopwords_removal = StopWordsRemover(inputCol='tokens',\n",
    "                                     outputCol='refined_tokens')\n",
    "refined_text_df = stopwords_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|              Review|Label|length|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|Brokeback Mountai...|  0.0|    40|[brokeback, mount...|[brokeback, mount...|          4|\n",
      "|These Harry Potte...|  0.0|    38|[these, harry, po...|[harry, potter, m...|          5|\n",
      "|The Da Vinci Code...|  0.0|    34|[the, da, vinci, ...|[da, vinci, code,...|          6|\n",
      "|So as felicia's m...|  1.0|    71|[so, as, felicia'...|[felicia's, mom, ...|          7|\n",
      "|man i loved broke...|  1.0|    31|[man, i, loved, b...|[man, loved, brok...|          4|\n",
      "|we're gonna like ...|  1.0|    51|[we're, gonna, li...|[gonna, like, wat...|          6|\n",
      "|DA VINCI CODE IS ...|  1.0|    26|[da, vinci, code,...|[da, vinci, code,...|          4|\n",
      "|Harry Potter is A...|  1.0|    66|[harry, potter, i...|[harry, potter, a...|          7|\n",
      "|, she helped me b...|  0.0|    72|[,, she, helped, ...|[,, helped, bobby...|          8|\n",
      "|This quiz sucks a...|  0.0|    47|[this, quiz, suck...|[quiz, sucks, har...|          7|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import rand, col\n",
    "\n",
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "refined_text_df = refined_text_df.withColumn('token_count', len_udf(col('refined_tokens')))\n",
    "refined_text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+-----+\n",
      "|      refined_tokens|token_count|            features|Label|\n",
      "+--------------------+-----------+--------------------+-----+\n",
      "|[da, vinci, code,...|          5|(2302,[0,1,4,43,2...|  1.0|\n",
      "|[first, clive, cu...|          9|(2302,[11,51,229,...|  1.0|\n",
      "|[liked, da, vinci...|          5|(2302,[0,1,4,53,3...|  1.0|\n",
      "|[liked, da, vinci...|          5|(2302,[0,1,4,53,3...|  1.0|\n",
      "|[liked, da, vinci...|          8|(2302,[0,1,4,53,6...|  1.0|\n",
      "|[even, exaggerati...|          6|(2302,[46,229,271...|  1.0|\n",
      "|[loved, da, vinci...|          8|(2302,[0,1,22,30,...|  1.0|\n",
      "|[thought, da, vin...|          7|(2302,[0,1,4,228,...|  1.0|\n",
      "|[da, vinci, code,...|          6|(2302,[0,1,4,33,2...|  1.0|\n",
      "|[thought, da, vin...|          7|(2302,[0,1,4,223,...|  1.0|\n",
      "+--------------------+-----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(inputCol='refined_tokens',\n",
    "                            outputCol='features')\n",
    "cv_text_df = count_vec.fit(refined_text_df).transform(refined_text_df)\n",
    "cv_text_df.select(['refined_tokens', 'token_count', 'features', 'Label']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text_df = cv_text_df.select(['features', 'token_count', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- token_count: integer (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_assembler = VectorAssembler(inputCols=['features', 'token_count'],\n",
    "                               outputCol='features_vec')\n",
    "model_text_df = df_assembler.transform(model_text_df)\n",
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "train_df, test_df = model_text_df.randomSplit([.75, .25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2907|\n",
      "|  0.0| 2318|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 1002|\n",
      "|  0.0|  763|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(2302,[0,1,4,5,12...|          7|  1.0|(2303,[0,1,4,5,12...|[-41.867867077536...|[6.56169982694168...|       1.0|\n",
      "|(2302,[0,1,4,5,64...|          6|  1.0|(2303,[0,1,4,5,64...|[-18.216681482391...|[1.22629723287176...|       1.0|\n",
      "|(2302,[0,1,4,5,30...|          5|  1.0|(2303,[0,1,4,5,30...|[-19.613921240322...|[3.03237030807226...|       1.0|\n",
      "|(2302,[0,1,4,5,44...|          5|  1.0|(2303,[0,1,4,5,44...|[-22.227997429774...|[2.22076892814667...|       1.0|\n",
      "|(2302,[0,1,4,5,65...|          5|  1.0|(2303,[0,1,4,5,65...|[-16.828934597592...|[4.91231883054462...|       1.0|\n",
      "|(2302,[0,1,4,5,82...|          6|  1.0|(2303,[0,1,4,5,82...|[-15.138940634494...|[2.66220506247678...|       1.0|\n",
      "|(2302,[0,1,4,12,1...|          8|  1.0|(2303,[0,1,4,12,1...|[-20.751077230289...|[9.72571782070369...|       1.0|\n",
      "|(2302,[0,1,4,12,1...|          5|  1.0|(2303,[0,1,4,12,1...|[-18.806596300753...|[6.79827925480307...|       1.0|\n",
      "|(2302,[0,1,4,12,2...|          7|  1.0|(2303,[0,1,4,12,2...|[-18.802534986030...|[6.82594534857137...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-19.070003986618...|[5.22399191915655...|       1.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features_vec',\n",
    "                             labelCol='Label').fit(train_df)\n",
    "results = log_reg.evaluate(test_df).predictions\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "tp = results[(results.Label == 1) & (results.prediction == 1)].count()\n",
    "tn = results[(results.Label == 0) & (results.prediction == 0)].count()\n",
    "fp = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
    "fn = results[(results.Label == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9930139720558883"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = float(tp) / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9688412852969815"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = float(tp) / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779036827195468"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = float(tp + tn) / (results.count())\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
